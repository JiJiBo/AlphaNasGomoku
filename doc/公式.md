 
---

**PUCT公式（节点选择阶段）**：

$$
\text{PUCT}(s,a) = \frac{W(s,a)}{N(s,a)} + c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sum_b N(s,b)}{1 + N(s,a)}
$$

---

**参数说明：**

- $W(s,a)$：节点的累积价值（Value）

- $N(s,a)$：节点访问次数

- $c_{\text{puct}}$：超参数，用于平衡探索与利用

- $P(s,a)$：动作概率（由神经网络策略网络输出）

- $\sum_b N(s,b)$：父节点所有子节点的总访问次数

---

**公式含义：**

1. **利用（Exploitation）**：$\frac{W(s,a)}{N(s,a)}$，代表当前节点的平均价值，偏向选择当前最优路径。

2. **探索（Exploration）**：$c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sum_b N(s,b)}{1 + N(s,a)}$，鼓励访问次数少但概率高的节点，避免局部最优。

3. **动态平衡**：访问次数越多，探索项越小，算法会逐渐收敛到最优路径。

 ---

## **1\. 策略损失（Policy Loss）**

策略网络（策略头）使用 **交叉熵损失**：

$$
L_{\text{policy}} = - \sum_a \pi(a|s) \, \log \pi_{\text{target}}(a|s)
$$

- $\pi(a|s)$：策略网络对动作 $a$ 在局面 $s$ 下的预测概率

- $\pi_{\text{target}}(a|s)$：目标策略概率（通常来自自我对弈或专家策略）

---

## **2\. 价值损失（Value Loss）**

价值网络（价值头）使用 **均方误差（MSE）**：

$$
L_{\text{value}} = \frac{1}{2} \big(v(s) - v_{\text{target}}(s)\big)^2
$$

- $v(s)$：当前网络对局面 $s$ 的胜率预测

- $v_{\text{target}}(s)$：目标胜率（自我对弈的实际胜负结果）

---

## **3\. 总损失（Combined Loss）**

策略损失和价值损失的加权和：

$$
L = L_{\text{policy}} + L_{\text{value}}
$$

> 权重可根据实验调整，例如策略损失权重为 1.0，价值损失权重为 0.5。
