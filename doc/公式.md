 
---

**PUCT公式（节点选择阶段）**：

$$
\text{PUCT}(s,a) = \frac{W(s,a)}{N(s,a)} + c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sum_b N(s,b)}{1 + N(s,a)}
$$

---

**参数说明：**

- $W(s,a)$：节点的累积价值（Value）

- $N(s,a)$：节点访问次数

- $c_{\text{puct}}$：超参数，用于平衡探索与利用

- $P(s,a)$：动作概率（由神经网络策略网络输出）

- $\sum_b N(s,b)$：父节点所有子节点的总访问次数

---

**公式含义：**

1. **利用（Exploitation）**：$\frac{W(s,a)}{N(s,a)}$，代表当前节点的平均价值，偏向选择当前最优路径。

2. **探索（Exploration）**：$c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sum_b N(s,b)}{1 + N(s,a)}$，鼓励访问次数少但概率高的节点，避免局部最优。

3. **动态平衡**：访问次数越多，探索项越小，算法会逐渐收敛到最优路径。

 ---

## **1\. 策略损失（Policy Loss）**

策略网络（策略头）使用 **交叉熵损失**：

$$
L_{\text{policy}} = - \sum_a \pi(a|s) \, \log \pi_{\text{target}}(a|s)
$$

- $\pi(a|s)$：策略网络对动作 $a$ 在局面 $s$ 下的预测概率

- $\pi_{\text{target}}(a|s)$：目标策略概率（通常来自自我对弈或专家策略）

---

## **2\. 价值损失（Value Loss）**

价值网络（价值头）使用 **均方误差（MSE）**：

$$
L_{\text{value}} = \frac{1}{2} \big(v(s) - v_{\text{target}}(s)\big)^2
$$

- $v(s)$：当前网络对局面 $s$ 的胜率预测

- $v_{\text{target}}(s)$：目标胜率（自我对弈的实际胜负结果）

---

## **3\. 总损失（Combined Loss）**

策略损失和价值损失的加权和：

$$
L = L_{\text{policy}} + L_{\text{value}}
$$

> 权重可根据实验调整，例如策略损失权重为 1.0，价值损失权重为 0.5。
---

# AlphaZero / MCTS 中的动作概率计算

```python
act_probs = softmax(1.0 / temp * np.log(np.array(visits) + 1e-10))
```

### 1️⃣ 参数说明

- `visits`：每个动作对应的访问次数 $N(s,a)$（MCTS搜索后的统计值）

- `np.log(np.array(visits) + 1e-10)`：

    - 对访问次数取对数，避免数值为 0 导致 `log(0)`

    - 使概率差异更加平滑

- `temp`（temperature）：

    - 温度参数，控制探索与确定性

    - 高温 `temp → ∞` → 选择更随机

    - 低温 `temp → 0` → 越接近贪婪选择（选访问最多的动作）

- `softmax(...)`：

    - 将对数访问次数转换为概率分布，使所有动作概率和为 1

---

### 2️⃣ 数学表达式

等价于：

$$
\pi(a|s) = \frac{\exp\left(\frac{\log(N(s,a) + \epsilon)}{T}\right)}{\sum_b \exp\left(\frac{\log(N(s,b) + \epsilon)}{T}\right)}
$$

- $N(s,a)$：动作 $a$ 的访问次数

- $T$：温度

- $\epsilon$ 防止 log(0)

---

### 3️⃣ 功能

- **将搜索结果转换为概率分布**，用于自我对弈采样动作。

- 温度 `temp` 可调节探索强度：

    - **训练初期**：使用较大 `temp`（更多探索）

    - **训练后期 / 评估**：使用小 `temp` 或接近 0（几乎总是选访问最多的动作）

---

### 4️⃣ 示例

假设 MCTS 搜索后访问次数：

```python
visits = [10, 5, 1]
temp = 1.0
```

计算步骤：

1. 对数：`log([10,5,1] + 1e-10) ≈ [2.30, 1.61, 0]`

2. softmax：

$$
\text{softmax}([2.30, 1.61, 0]) \approx [0.61, 0.33, 0.06]
$$

→ 得到动作概率分布，用于选择动作。
